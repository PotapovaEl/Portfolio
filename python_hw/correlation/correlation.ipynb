{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 Задача - ответить на вопрос есть ли связь между жёсткостью воды и средней годовой смертностью?\n",
    "Построить точечный график\n",
    "Рассчитать коэффициенты корреляции Пирсона и Спирмена\n",
    "Построить модель линейной регрессии\n",
    "Рассчитать коэффициент детерминации\n",
    "Вывести график остатков\n",
    "In [81]:\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "In [2]:\n",
    "df = pd.read_csv('water.csv', index_col=0)\n",
    "df.head()\n",
    "Out[2]:\n",
    "location\ttown\tmortality\thardness\n",
    "1\tSouth\tBath\t1247\t105\n",
    "2\tNorth\tBirkenhead\t1668\t17\n",
    "3\tSouth\tBirmingham\t1466\t5\n",
    "4\tNorth\tBlackburn\t1800\t14\n",
    "5\tNorth\tBlackpool\t1609\t18\n",
    "In [3]:\n",
    "df.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 61 entries, 1 to 61\n",
    "Data columns (total 4 columns):\n",
    " #   Column     Non-Null Count  Dtype \n",
    "---  ------     --------------  ----- \n",
    " 0   location   61 non-null     object\n",
    " 1   town       61 non-null     object\n",
    " 2   mortality  61 non-null     int64 \n",
    " 3   hardness   61 non-null     int64 \n",
    "dtypes: int64(2), object(2)\n",
    "memory usage: 2.4+ KB\n",
    "In [41]:\n",
    "df.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[41]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "Глядя на график, можно предположить, что связь есть, зависимость обратная. Максимальное из имеющихся значение смертности (и довольно крупное скопление точек, обозначающих высокие показатели смертности) отмечено при низком значении показателя жесткости воды, минимальное из имеющихся значение смертности достигается при максимальном значении показателя жесткости.\n",
    "\n",
    "In [5]:\n",
    "# коэффициент Пирсона\n",
    "df[['hardness', 'mortality']].corr()\n",
    "Out[5]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.654849\n",
    "mortality\t-0.654849\t1.000000\n",
    "In [6]:\n",
    "# коэффициент Спирмана\n",
    "df[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[6]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.631665\n",
    "mortality\t-0.631665\t1.000000\n",
    "Полученные значения коэффициентов корреляции близки по значению (Пирсон = -0,655, Спирмен = -0,632), оба указывают на наличие не очень сильной обратной взаимосвязи.\n",
    "\n",
    "In [7]:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "In [24]:\n",
    "# если есть зависимость между переменными, то логично предположить, что жесткость воды влияет на смертность, а не наоборот.\n",
    "x = df[['hardness']]\n",
    "y = df['mortality']\n",
    "In [62]:\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.31, random_state=146)\n",
    "In [63]:\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred\n",
    "Out[63]:\n",
    "array([1399.53149147, 1626.98697941, 1611.4078364 , 1608.2920078 ,\n",
    "       1408.87897728, 1499.23800673, 1620.75532221, 1626.98697941,\n",
    "       1514.81714974, 1430.68977749, 1608.2920078 , 1430.68977749,\n",
    "       1514.81714974, 1352.79406245, 1620.75532221, 1324.75160503,\n",
    "       1271.7825188 , 1440.0372633 , 1623.87115081])\n",
    "In [64]:\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, y_pred, c='r')\n",
    "Out[64]:\n",
    "[<matplotlib.lines.Line2D at 0x1be46698d48>]\n",
    "\n",
    "In [65]:\n",
    "# коэффициент детерминации\n",
    "model.score(x_test, y_test)\n",
    "Out[65]:\n",
    "0.3264271953972949\n",
    "Коэффициент детерминации не близок к 1, т.е. построенная регрессия не так хорошо объясняет зависимость данных. Стоит еще учитывать, что у нас не очень много имеющихся данных (61 запись в датасете), и построить на них хорошую модель может быть проблематично.\n",
    "\n",
    "Во всяком случае построенная модель так же демонстрирует обратную зависимость смертности от жесткости воды.\n",
    "\n",
    "Можно попробовать улучшить качество модели. Так как данных мало, разные варианты разбиения выборки на тренировочную и тестовую существенного повышения значения коэффициента детерминации не дают. Можно изменить значения параметра random_state и посмотреть, будет ли меняться коэффициент. Построим модель при значениях random_state от 1 до 9999 и выберем в итоге наилучшее значение, которое даст максимальный коэффициент детерминации\n",
    "\n",
    "In [61]:\n",
    "lst_index = []\n",
    "model = LinearRegression()\n",
    "for i in range(1, 10000):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.31, random_state=i)\n",
    "    model.fit(x_train, y_train)\n",
    "    if model.score(x_test, y_test) > 0.6:\n",
    "        lst_index.append((model.score(x_test, y_test), i))\n",
    "        \n",
    "\n",
    "print(max(lst_index, key=lambda item: item[0]))\n",
    "(0.7570280355687193, 6383)\n",
    "In [66]:\n",
    "# построим модель с полученным параметром = 6383\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.31, random_state=6383)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, y_pred, c='r')\n",
    "Out[66]:\n",
    "[<matplotlib.lines.Line2D at 0x1be467092c8>]\n",
    "\n",
    "In [67]:\n",
    "model.score(x_test, y_test)\n",
    "Out[67]:\n",
    "0.7570280355687193\n",
    "Видим, что разбиение получили немного другое. Но это дало существенный прирост коэффициента детерминации. Теперь он приближен к 1, значит, такая модель лучше объясняет нашу зависимость данных.\n",
    "\n",
    "Стоит учитывать, что на среднегодовую смертность населения влияет множество факторов, и жесткость воды может быть одним из них, а может и не быть. Однако, во всяком случае мы видим не причинно-следственную, но статистическую связь между поведением двух рассматриваемых величин.\n",
    "\n",
    "In [85]:\n",
    "# теперь посмотрим на остатки\n",
    "residuals = y_train-model.predict(x_train)\n",
    "sns.residplot(x=x_train, y=y_train-model.predict(x_train), lowess=True)\n",
    "Out[85]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "Видим, что остатки распределены достаточно хаотично. В чередовании знаков не прослеживается закономерностей. Это говорит о корректности построенной модели\n",
    "\n",
    "In [117]:\n",
    "sns.distplot(residuals)\n",
    "Out[117]:\n",
    "<AxesSubplot:xlabel='mortality'>\n",
    "\n",
    "2 Задание - cохраняется ли аналогичная зависимость для северных и южных городов по отдельности?\n",
    "Разделить данные на 2 группы\n",
    "Повторить аналогичные шаги из пункта 1 для каждой группы по отдельности\n",
    "In [97]:\n",
    "# разделим наши данные по территориям север-юг\n",
    "df_north = df[df['location']=='North']\n",
    "df_south = df[df['location']=='South']\n",
    "In [98]:\n",
    "df_north.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 35 entries, 2 to 61\n",
    "Data columns (total 4 columns):\n",
    " #   Column     Non-Null Count  Dtype \n",
    "---  ------     --------------  ----- \n",
    " 0   location   35 non-null     object\n",
    " 1   town       35 non-null     object\n",
    " 2   mortality  35 non-null     int64 \n",
    " 3   hardness   35 non-null     int64 \n",
    "dtypes: int64(2), object(2)\n",
    "memory usage: 1.4+ KB\n",
    "In [100]:\n",
    "df_south.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 26 entries, 1 to 60\n",
    "Data columns (total 4 columns):\n",
    " #   Column     Non-Null Count  Dtype \n",
    "---  ------     --------------  ----- \n",
    " 0   location   26 non-null     object\n",
    " 1   town       26 non-null     object\n",
    " 2   mortality  26 non-null     int64 \n",
    " 3   hardness   26 non-null     int64 \n",
    "dtypes: int64(2), object(2)\n",
    "memory usage: 1.0+ KB\n",
    "Посмотрим на данные по северным городам:\n",
    "\n",
    "In [101]:\n",
    "df_north.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[101]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "In [104]:\n",
    "# коэффициент Пирсона\n",
    "df_north[['hardness', 'mortality']].corr()\n",
    "Out[104]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.368598\n",
    "mortality\t-0.368598\t1.000000\n",
    "In [105]:\n",
    "# коэффициент Спирмана\n",
    "df_north[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[105]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.404208\n",
    "mortality\t-0.404208\t1.000000\n",
    "Глядя на график можно предположить, что есть некая обратная зависимость между показателями, но достаточно слабая. Менее выраженная, чем в случае с полным датасетом. Это подтверждают и значения коэффициентов Пирсона и Спирмена\n",
    "\n",
    "In [134]:\n",
    "x = df_north[['hardness']]\n",
    "y = df_north['mortality']\n",
    "In [135]:\n",
    "# подберем параметр random_state для модели\n",
    "lst_index = []\n",
    "model_north = LinearRegression()\n",
    "for i in range(1, 10000):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=i)\n",
    "    model_north.fit(x_train, y_train)\n",
    "    if model_north.score(x_test, y_test) > 0.3:\n",
    "        lst_index.append((model_north.score(x_test, y_test), i))\n",
    "        \n",
    "\n",
    "print(max(lst_index, key=lambda item: item[0]))\n",
    "(0.5033286961083134, 7174)\n",
    "In [136]:\n",
    "# построим модель\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=7174)\n",
    "model_north.fit(x_train, y_train)\n",
    "y_pred_north = model_north.predict(x_test)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, y_pred_north, c='r')\n",
    "Out[136]:\n",
    "[<matplotlib.lines.Line2D at 0x1be4b6e3488>]\n",
    "\n",
    "In [137]:\n",
    "model_north.score(x_test, y_test)\n",
    "Out[137]:\n",
    "0.5033286961083134\n",
    "Значение коэффициента детерминации ощутимо меньше, чем в прошлый раз - модель мы получили менее точную. Но с учетом того, насколько маленький у нас теперь датасет, предполагаю, что это ожидаемо.\n",
    "\n",
    "In [114]:\n",
    "residuals = y_train-model_north.predict(x_train)\n",
    "sns.residplot(x=x_train, y=y_train-model_north.predict(x_train), lowess=True)\n",
    "Out[114]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "In [116]:\n",
    "sns.distplot(residuals)\n",
    "Out[116]:\n",
    "<AxesSubplot:xlabel='mortality'>\n",
    "\n",
    "В целом, получили картину, сопоставимую с полным датафреймом. Модель предполагает наличие обратной зависимости смертности от жесткости воды. Остатки распределены хаотично.\n",
    "\n",
    "Теперь взглянем на южные города:\n",
    "\n",
    "In [120]:\n",
    "df_south.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[120]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "In [121]:\n",
    "# коэффициент Пирсона\n",
    "df_south[['hardness', 'mortality']].corr()\n",
    "Out[121]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.602153\n",
    "mortality\t-0.602153\t1.000000\n",
    "In [122]:\n",
    "# коэффициент Спирмана\n",
    "df_south[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[122]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.595723\n",
    "mortality\t-0.595723\t1.000000\n",
    "На графике мало точек и на первый взгляд кажется, что они разбросаны достаточно хаотично, наша предполагаемая зависимость менее очевидна. Однако математически коэффициенты Пирсона и Спирмена предполагают с большей степенью уверенности, чем в случае с северными городами, наличие обратной взаимосвязи.\n",
    "\n",
    "In [123]:\n",
    "x = df_south[['hardness']]\n",
    "y = df_south['mortality']\n",
    "In [124]:\n",
    "# подберем параметр random_state для модели\n",
    "lst_index = []\n",
    "model_south = LinearRegression()\n",
    "for i in range(1, 10000):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=i)\n",
    "    model_south.fit(x_train, y_train)\n",
    "    if model_south.score(x_test, y_test) > 0.3:\n",
    "        lst_index.append((model_south.score(x_test, y_test), i))\n",
    "        \n",
    "\n",
    "print(max(lst_index, key=lambda item: item[0]))\n",
    "(0.8070290759570379, 9323)\n",
    "In [126]:\n",
    "# построим модель\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=9323)\n",
    "model_south.fit(x_train, y_train)\n",
    "y_pred_south = model_south.predict(x_test)\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, y_pred_south, c='r')\n",
    "Out[126]:\n",
    "[<matplotlib.lines.Line2D at 0x1be4b97ec08>]\n",
    "\n",
    "In [127]:\n",
    "model_south.score(x_test, y_test)\n",
    "Out[127]:\n",
    "0.8070290759570379\n",
    "In [128]:\n",
    "residuals = y_train-model_south.predict(x_train)\n",
    "sns.residplot(x=x_train, y=y_train-model_south.predict(x_train), lowess=True)\n",
    "Out[128]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "In [130]:\n",
    "sns.distplot(residuals, bins=10)\n",
    "Out[130]:\n",
    "<AxesSubplot:xlabel='mortality'>\n",
    "\n",
    "И здесь наблюдаем аналогичное поведение модели и остатков. Сравнивая графики моделей для северных и южных городов между собой, можно сделать предположение, что для южных городов связь между рассматриваемыми переменными получается несколько более сильная, чем для северных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "Задание 1\n",
    "Есть ли связь между жёсткостью воды и средней годовой смертностью?\n",
    "\n",
    "Построить точечный график\n",
    "Рассчитать коэффициенты корреляции Пирсона и Спирмена\n",
    "Построить модель линейной регрессии\n",
    "Рассчитать коэффициент детерминации\n",
    "Вывести график остатков\n",
    "Примечание\n",
    "Необходимо обратить внимание, что исходный датасет очень мал. В этой связи выводы, сделанные на его основании, не могут считаться надеждными. Тем более, что обучение модели линейной регрессии проводится на неполных (тренировочных) данных.\n",
    "\n",
    "При разбиении данных на две части по признаку location ситуация усугубляется.\n",
    "\n",
    "In [7]:\n",
    "df_water = pd.read_csv('water.csv')\n",
    "df_water.head()\n",
    "Out[7]:\n",
    "Unnamed: 0\tlocation\ttown\tmortality\thardness\n",
    "0\t1\tSouth\tBath\t1247\t105\n",
    "1\t2\tNorth\tBirkenhead\t1668\t17\n",
    "2\t3\tSouth\tBirmingham\t1466\t5\n",
    "3\t4\tNorth\tBlackburn\t1800\t14\n",
    "4\t5\tNorth\tBlackpool\t1609\t18\n",
    "In [8]:\n",
    "df_water.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[8]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "Заметна обратная линейная корреляция между значениями жесткости воды и значениями смертности.\n",
    "\n",
    "In [9]:\n",
    "df_water[['hardness', 'mortality']].corr()\n",
    "Out[9]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.654849\n",
    "mortality\t-0.654849\t1.000000\n",
    "In [10]:\n",
    "df_water[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[10]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.631665\n",
    "mortality\t-0.631665\t1.000000\n",
    "Обратная корреляция подтверждается расчетами коэффициентов корреляции Пирсона и Спирмана. Их значения довольно близки, однако в данном случае в качестве окончательного предлагается взять имено коэффициент Пирсона, так как значения жесткости воды и смертности по своей сути являются непрерывными.\n",
    "\n",
    "In [11]:\n",
    "X = df_water[['hardness']]\n",
    "y = df_water['mortality']\n",
    "In [12]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "In [13]:\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[13]:\n",
    "LinearRegression()\n",
    "In [14]:\n",
    "model.score(X_test, y_test)\n",
    "Out[14]:\n",
    "0.5046490611017092\n",
    "Значение коэффициента детерминации не очень велико, однако всё ещё значимо. Это говорит о предпочтительность использования модели линейной регрессии для прогнозирования новых значений.\n",
    "\n",
    "In [15]:\n",
    "y_pred = model.predict(X_test)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, c='r')\n",
    "Out[15]:\n",
    "[<matplotlib.lines.Line2D at 0x1ab4243ccc8>]\n",
    "\n",
    "Линия линейной регрессии визуально неплохо апроксимирует значения точек на графике. Стоит отметить, что для более высоких значений плотности воды апроксимация визуально более точна.\n",
    "\n",
    "In [16]:\n",
    "X_const = sm.add_constant(X_train)\n",
    "In [17]:\n",
    "model = sm.OLS(y_train, X_const)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:              mortality   R-squared:                       0.397\n",
    "Model:                            OLS   Adj. R-squared:                  0.382\n",
    "Method:                 Least Squares   F-statistic:                     26.31\n",
    "Date:                Tue, 15 Dec 2020   Prob (F-statistic):           7.83e-06\n",
    "Time:                        10:36:41   Log-Likelihood:                -269.10\n",
    "No. Observations:                  42   AIC:                             542.2\n",
    "Df Residuals:                      40   BIC:                             545.7\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const       1668.9723     36.543     45.671      0.000    1595.115    1742.829\n",
    "hardness      -3.1317      0.611     -5.130      0.000      -4.366      -1.898\n",
    "==============================================================================\n",
    "Omnibus:                        0.116   Durbin-Watson:                   2.428\n",
    "Prob(Omnibus):                  0.944   Jarque-Bera (JB):                0.323\n",
    "Skew:                           0.048   Prob(JB):                        0.851\n",
    "Kurtosis:                       2.581   Cond. No.                         94.3\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "In [18]:\n",
    "plt.scatter(X_const.iloc[:, 1], results.resid)\n",
    "Out[18]:\n",
    "<matplotlib.collections.PathCollection at 0x1ab42b6c2c8>\n",
    "\n",
    "Остатки распределены довольно хаотично и симмитрично относительно 0, что говорит о том, что модель подобрана правильно.\n",
    "\n",
    "Задание 2\n",
    "Сохраняется ли аналогичная зависимость для северных и южных городов по отдельности?\n",
    "\n",
    "Разделить данные на 2 группы\n",
    "Повторить аналогичные шаги из пункта 1 для каждой группы по отдельности\n",
    "Для южных городов\n",
    "In [19]:\n",
    "df_water_S = df_water[df_water['location'] == 'South']\n",
    "In [20]:\n",
    "df_water_S.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[20]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "Только для южных городов обратная корреляция всё ещё прослеживается, однако несколько менее ярко выражено.\n",
    "\n",
    "In [21]:\n",
    "df_water_S[['hardness', 'mortality']].corr()\n",
    "Out[21]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.602153\n",
    "mortality\t-0.602153\t1.000000\n",
    "In [22]:\n",
    "df_water_S[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[22]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.595723\n",
    "mortality\t-0.595723\t1.000000\n",
    "Коэффициенты корреляции подтверждают выводы, сделанные на основе графика - оба коэффициента ниже, чем для полной выборки.\n",
    "\n",
    "In [23]:\n",
    "X = df_water_S[['hardness']]\n",
    "y = df_water_S['mortality']\n",
    "In [24]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "In [25]:\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[25]:\n",
    "LinearRegression()\n",
    "In [26]:\n",
    "model.score(X_test, y_test)\n",
    "Out[26]:\n",
    "-2.5519223012352077\n",
    "Коэффициент детерминации крайне низок, что говорит об ужасном прогностическом качестве модели.\n",
    "\n",
    "In [27]:\n",
    "y_pred = model.predict(X_test)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, c='r')\n",
    "Out[27]:\n",
    "[<matplotlib.lines.Line2D at 0x1ab42c2e948>]\n",
    "\n",
    "Действительно, визуально линия регресии довольно плохо апроксимирует точки из тестового набора.\n",
    "\n",
    "In [28]:\n",
    "X_const = sm.add_constant(X_train)\n",
    "In [29]:\n",
    "model = sm.OLS(y_train, X_const)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:              mortality   R-squared:                       0.636\n",
    "Model:                            OLS   Adj. R-squared:                  0.614\n",
    "Method:                 Least Squares   F-statistic:                     28.00\n",
    "Date:                Tue, 15 Dec 2020   Prob (F-statistic):           7.30e-05\n",
    "Time:                        10:36:42   Log-Likelihood:                -106.97\n",
    "No. Observations:                  18   AIC:                             217.9\n",
    "Df Residuals:                      16   BIC:                             219.7\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const       1640.2205     54.471     30.112      0.000    1524.747    1755.694\n",
    "hardness      -3.5134      0.664     -5.292      0.000      -4.921      -2.106\n",
    "==============================================================================\n",
    "Omnibus:                        0.131   Durbin-Watson:                   2.022\n",
    "Prob(Omnibus):                  0.937   Jarque-Bera (JB):                0.042\n",
    "Skew:                           0.035   Prob(JB):                        0.979\n",
    "Kurtosis:                       2.774   Cond. No.                         194.\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1604: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
    "  \"anyway, n=%i\" % int(n))\n",
    "In [30]:\n",
    "plt.scatter(X_const.iloc[:, 1], results.resid)\n",
    "Out[30]:\n",
    "<matplotlib.collections.PathCollection at 0x1ab43d28848>\n",
    "\n",
    "На графике остатков точки довольно сильно сгруппированны в верхне части графика, что также говорит о низком качестве данной модели линейной регресии.\n",
    "\n",
    "Для северных городов\n",
    "In [31]:\n",
    "df_water_N = df_water[df_water['location'] == 'North']\n",
    "In [32]:\n",
    "df_water_N.plot(kind='scatter', x='hardness', y='mortality')\n",
    "Out[32]:\n",
    "<AxesSubplot:xlabel='hardness', ylabel='mortality'>\n",
    "\n",
    "Для северных городов также наблюдается обратная корреляция, однако ещё более слабая, чем в предыдущих случаях. Визиуально постронные точки должны не так плохо аппроксимироваться чем-то вроде графиков $y = 1/x$ и $y = -ln(x)$\n",
    "\n",
    "In [33]:\n",
    "df_water_N[['hardness', 'mortality']].corr()\n",
    "Out[33]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.368598\n",
    "mortality\t-0.368598\t1.000000\n",
    "In [34]:\n",
    "df_water_N[['hardness', 'mortality']].corr(method='spearman')\n",
    "Out[34]:\n",
    "hardness\tmortality\n",
    "hardness\t1.000000\t-0.404208\n",
    "mortality\t-0.404208\t1.000000\n",
    "Значения коэффициентов корреляции соответствуют выводам, сделанным на основе графика.\n",
    "\n",
    "In [35]:\n",
    "X = df_water_N[['hardness']]\n",
    "y = df_water_N['mortality']\n",
    "In [36]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "In [37]:\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[37]:\n",
    "LinearRegression()\n",
    "In [38]:\n",
    "model.score(X_test, y_test)\n",
    "Out[38]:\n",
    "0.018885304285745863\n",
    "Значение коэффиицентов детерминации близко к нулю, что говорит о невысоком прогностическом качестве модели. Самая простая модели оценки через среднее дала бы похожий результат.\n",
    "\n",
    "In [39]:\n",
    "y_pred = model.predict(X_test)\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, c='r')\n",
    "Out[39]:\n",
    "[<matplotlib.lines.Line2D at 0x1ab42c9af88>]\n",
    "\n",
    "Прямая довольно плохо аппроксимирует точки на графике. Стоит отметить, что в данной тестовой выборке особенно мало точек для построенения каких-либо выводов на их основе.\n",
    "\n",
    "In [40]:\n",
    "X_const = sm.add_constant(X_train)\n",
    "In [41]:\n",
    "model = sm.OLS(y_train, X_const)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:              mortality   R-squared:                       0.193\n",
    "Model:                            OLS   Adj. R-squared:                  0.157\n",
    "Method:                 Least Squares   F-statistic:                     5.269\n",
    "Date:                Tue, 15 Dec 2020   Prob (F-statistic):             0.0316\n",
    "Time:                        10:36:42   Log-Likelihood:                -147.20\n",
    "No. Observations:                  24   AIC:                             298.4\n",
    "Df Residuals:                      22   BIC:                             300.7\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const       1688.1502     37.137     45.457      0.000    1611.133    1765.167\n",
    "hardness      -1.9769      0.861     -2.295      0.032      -3.763      -0.191\n",
    "==============================================================================\n",
    "Omnibus:                        3.045   Durbin-Watson:                   1.872\n",
    "Prob(Omnibus):                  0.218   Jarque-Bera (JB):                1.526\n",
    "Skew:                          -0.279   Prob(JB):                        0.466\n",
    "Kurtosis:                       1.898   Cond. No.                         67.4\n",
    "==============================================================================\n",
    "\n",
    "Notes:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "In [42]:\n",
    "plt.scatter(X_const.iloc[:, 1], results.resid)\n",
    "Out[42]:\n",
    "<matplotlib.collections.PathCollection at 0x1ab43e7f0c8>\n",
    "\n",
    "Точки в значительной степени сгруппированы около низких значений жесткости воды. Для высоких значений наблюдается смещение точек в верхнюю часть графика. Распределение точек не хаотично, что говорит о том, что модель линейной регрессии в данном случае - не самый удачный выбор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-bb66166f51a0>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-bb66166f51a0>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    In [54]:\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "In [54]:\n",
    "water_df = pd.read_csv('water.csv')\n",
    "water_df.head(20)\n",
    "Out[54]:\n",
    "Unnamed: 0\tlocation\ttown\tmortality\thardness\n",
    "0\t1\tSouth\tBath\t1247\t105\n",
    "1\t2\tNorth\tBirkenhead\t1668\t17\n",
    "2\t3\tSouth\tBirmingham\t1466\t5\n",
    "3\t4\tNorth\tBlackburn\t1800\t14\n",
    "4\t5\tNorth\tBlackpool\t1609\t18\n",
    "5\t6\tNorth\tBolton\t1558\t10\n",
    "6\t7\tNorth\tBootle\t1807\t15\n",
    "7\t8\tSouth\tBournemouth\t1299\t78\n",
    "8\t9\tNorth\tBradford\t1637\t10\n",
    "9\t10\tSouth\tBrighton\t1359\t84\n",
    "10\t11\tSouth\tBristol\t1392\t73\n",
    "11\t12\tNorth\tBurnley\t1755\t12\n",
    "12\t13\tSouth\tCardiff\t1519\t21\n",
    "13\t14\tSouth\tCoventry\t1307\t78\n",
    "14\t15\tSouth\tCroydon\t1254\t96\n",
    "15\t16\tNorth\tDarlington\t1491\t20\n",
    "16\t17\tNorth\tDerby\t1555\t39\n",
    "17\t18\tNorth\tDoncaster\t1428\t39\n",
    "18\t19\tSouth\tEast Ham\t1318\t122\n",
    "19\t20\tSouth\tExeter\t1260\t21\n",
    "In [6]:\n",
    "plt.scatter(water_df['hardness'], water_df['mortality'])\n",
    "Out[6]:\n",
    "<matplotlib.collections.PathCollection at 0x28db83f6a90>\n",
    "\n",
    "In [7]:\n",
    "water_df[['mortality', 'hardness']].corr()\n",
    "Out[7]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.654849\n",
    "hardness\t-0.654849\t1.000000\n",
    "In [8]:\n",
    "water_df[['mortality', 'hardness']].corr(method='spearman')\n",
    "Out[8]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.631665\n",
    "hardness\t-0.631665\t1.000000\n",
    "На удивление оба метода показывают чрезвычайно близкую корелляцию - чем выше жесткость воды, тем ниже смертность\n",
    "\n",
    "In [20]:\n",
    "x = water_df[['mortality']]\n",
    "y = water_df[['hardness']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "water_model = LinearRegression().fit(x_train, y_train)\n",
    "In [21]:\n",
    "r_sq = water_model.score(x, y)\n",
    "print('coefficient of determination:', r_sq)\n",
    "coefficient of determination: 0.42685578465984875\n",
    "Коэффициент детерминации у нас маловат, меньше 50%\n",
    "\n",
    "In [22]:\n",
    "y_pred = model.predict(x_test)\n",
    "In [23]:\n",
    "plt.scatter(x_test, y_test)\n",
    "plt.plot(x_test, y_pred, c='r')\n",
    "Out[23]:\n",
    "[<matplotlib.lines.Line2D at 0x28db89ac1f0>]\n",
    "\n",
    "Чет как-то немного уныло\n",
    "\n",
    "In [29]:\n",
    "plt.scatter(y_test,  y_pred - y_test, c='blue', marker='+', label='Training data')\n",
    "plt.scatter(y_pred,  y_pred - y_test, c='lightgreen', marker='*', label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='lower left')\n",
    "Out[29]:\n",
    "<matplotlib.legend.Legend at 0x28db8a30250>\n",
    "\n",
    "Посмотрим, что у нас с городами по северу и югу\n",
    "\n",
    "In [31]:\n",
    "mask_s = water_df['location'] == 'South'\n",
    "mask_n = water_df['location'] == 'North'\n",
    "water_s_df = water_df[mask_s]\n",
    "water_n_df = water_df[mask_n]\n",
    "In [32]:\n",
    "plt.scatter(water_n_df['hardness'], water_n_df['mortality'])\n",
    "Out[32]:\n",
    "<matplotlib.collections.PathCollection at 0x28db8b81fa0>\n",
    "\n",
    "In [33]:\n",
    "plt.scatter(water_s_df['hardness'], water_s_df['mortality'])\n",
    "Out[33]:\n",
    "<matplotlib.collections.PathCollection at 0x28db8be11c0>\n",
    "\n",
    "о как интересно! оказывается на Юге у нас все хаотично, да и на Севере не все так однозначно. Посмотрим на корелляцию\n",
    "\n",
    "In [34]:\n",
    "water_n_df[['mortality', 'hardness']].corr()\n",
    "Out[34]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.368598\n",
    "hardness\t-0.368598\t1.000000\n",
    "In [35]:\n",
    "water_s_df[['mortality', 'hardness']].corr()\n",
    "Out[35]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.602153\n",
    "hardness\t-0.602153\t1.000000\n",
    "на Юге, что забавно, корелляция в два раза выше, чем на Севере, но все равно меньше чем на общем наборе. Ну и опасная вещь - статистика\n",
    "\n",
    "In [36]:\n",
    "len(water_n_df)\n",
    "Out[36]:\n",
    "35\n",
    "In [37]:\n",
    "len(water_s_df)\n",
    "Out[37]:\n",
    "26\n",
    "а данных по Югу то у нас и меньше внезапно. Модет выкинуть случайный значения и сравнять их, а потом проверить\n",
    "\n",
    "In [75]:\n",
    "trials = len(water_n_df) - len(water_s_df)\n",
    "numbers = list(water_n_df.index)\n",
    "shuffle(numbers)\n",
    "excess = numbers[:trials]\n",
    "water_n_df = water_n_df.drop(index=excess)\n",
    "In [76]:\n",
    "plt.scatter(water_n_df['hardness'], water_n_df['mortality'])\n",
    "Out[76]:\n",
    "<matplotlib.collections.PathCollection at 0x28db8e77640>\n",
    "\n",
    "In [77]:\n",
    "water_n_df[['mortality', 'hardness']].corr()\n",
    "Out[77]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.329337\n",
    "hardness\t-0.329337\t1.000000\n",
    "хмм, а результат-то практически не изменился, разве что корелляция уменьшилась еще больше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Тема: \"Корреляция данных\"\n",
    "Задача \n",
    "ответить на вопрос есть ли связь между жёсткостью воды и средней годовой смертностью?\n",
    "\n",
    "Задание 1 \n",
    "Построить точечный график+\n",
    "Рассчитать коэффициенты корреляции Пирсона и Спирмена+\n",
    "Построить модель линейной регрессии\n",
    "Рассчитать коэффициент детерминации\n",
    "Вывести график остатков\n",
    "\n",
    "Задание 2\n",
    "Сохраняется ли аналогичная зависимость для северных и южных городов по отдельности?\n",
    "Разделить данные на 2 группы\n",
    "Повторить аналогичные шаги из пункта 1 для каждой группы по отдельности\"\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "df = pd.read_csv('water.csv')\n",
    "df.head(5)\n",
    "Out[211]:\n",
    "Unnamed: 0\tlocation\ttown\tmortality\thardness\n",
    "0\t1\tSouth\tBath\t1247\t105\n",
    "1\t2\tNorth\tBirkenhead\t1668\t17\n",
    "2\t3\tSouth\tBirmingham\t1466\t5\n",
    "3\t4\tNorth\tBlackburn\t1800\t14\n",
    "4\t5\tNorth\tBlackpool\t1609\t18\n",
    "In [ ]:\n",
    "#Ключевыми атрибутами являются mortality – смертность и hardness – жесткость, эти столбцы числовые, количественные значения\n",
    "#А теперь приступим к изучению визуализации данных в pandas.\n",
    "In [228]:\n",
    "rcParams['figure.figsize'] = 12,5\n",
    "In [229]:\n",
    "df.plot(kind='scatter', x='mortality', y='hardness') \n",
    "# построим точечный график зависимости по оси  x - расположим смертность, по оси y возьмем жесткость воды.\n",
    "# Смотрим на визуализацю и выдвигаем гипотезу взаимосвязи этих величин: \n",
    "# взаимосвязь между жесткостью воды и смертностью - явно не прослеживается\n",
    "# в общих чертах видим обратную корреляцию. Т.е пока прямой зависимости нет,\n",
    "#чтобы это подтвердить или опровергнуть расчитаем коэффициэнты Спирмена и Кенделла\n",
    "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
    "Out[229]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x15fd7af0>\n",
    "\n",
    "In [230]:\n",
    "sns.pairplot(df) # используем pairplot для изучения взаимосвязи множественных признаков\n",
    "Out[230]:\n",
    "<seaborn.axisgrid.PairGrid at 0x163c8730>\n",
    "\n",
    "In [231]:\n",
    "d = df.groupby('hardness').mean()\n",
    "d.plot()\n",
    "Out[231]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x17791610>\n",
    "\n",
    "In [232]:\n",
    "d = df.groupby('mortality').mean()\n",
    "d.plot()\n",
    "Out[232]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x17ae7670>\n",
    "\n",
    "In [233]:\n",
    "# Рассчитать коэффициент корреляции Пирсона \n",
    "df[['mortality','hardness']].corr\n",
    "Out[233]:\n",
    "<bound method DataFrame.corr of     mortality  hardness\n",
    "0        1247       105\n",
    "1        1668        17\n",
    "2        1466         5\n",
    "3        1800        14\n",
    "4        1609        18\n",
    "..        ...       ...\n",
    "56       1527        60\n",
    "57       1627        53\n",
    "58       1486       122\n",
    "59       1485        81\n",
    "60       1378        71\n",
    "\n",
    "[61 rows x 2 columns]>\n",
    "In [234]:\n",
    "# Рассчитать коэффициент корреляции Спирмена\n",
    "df[['mortality','hardness']].corr(method = 'spearman')\n",
    "#Это ковариация двух переменных поделить на их дисперсии.\n",
    "#Т.к Величина коэффициента корреляции заключена в пределах -1 <= r <= 0, \n",
    "# то имеет место обратная связь, при увеличении значений одной из величин значения другой имеют тенденцию к уменьшению\n",
    "Out[234]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.631665\n",
    "hardness\t-0.631665\t1.000000\n",
    "In [225]:\n",
    "# Рассчитать коэффициент корреляции Кенделла\n",
    "df[['mortality','hardness']].corr(method = 'kendall')\n",
    "#Это ковариация двух переменных поделить на их дисперсии.\n",
    "#Т.к Величина коэффициента корреляции заключена в пределах -1 <= r <= 0, \n",
    "# то имеет место обратная связь, при увеличении значений одной из величин значения другой имеют тенденцию к уменьшению\n",
    "Out[225]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.453753\n",
    "hardness\t-0.453753\t1.000000\n",
    "In [235]:\n",
    "df.corr() # на Unnamed  не обращаем внимания т.к это порядковая величина\n",
    "Out[235]:\n",
    "Unnamed: 0\tmortality\thardness\n",
    "Unnamed: 0\t1.000000\t0.077133\t0.123075\n",
    "mortality\t0.077133\t1.000000\t-0.654849\n",
    "hardness\t0.123075\t-0.654849\t1.000000\n",
    "In [238]:\n",
    "# строим тепловую карту\n",
    "sns.set(rc={'figure.figsize':(5.0,4.0)})\n",
    "sns.heatmap(df.corr(),annot= True)\n",
    "Out[238]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x17b7c5b0>\n",
    "\n",
    "In [239]:\n",
    "# Построим модель линейной регрессии\n",
    "# Реализация простой линейной регрессии начинается с заданным набором пар x-y. Эти пары – результаты наблюдений. \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "In [240]:\n",
    "X = df[['mortality']]\n",
    "y = df['hardness']\n",
    "In [241]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "In [242]:\n",
    "X_train.shape\n",
    "Out[242]:\n",
    "(42, 1)\n",
    "In [243]:\n",
    "y_train.shape\n",
    "Out[243]:\n",
    "(42,)\n",
    "In [244]:\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[244]:\n",
    "LinearRegression()\n",
    "In [245]:\n",
    "model.coef_\n",
    "Out[245]:\n",
    "array([-0.12670202])\n",
    "In [246]:\n",
    "model.intercept_\n",
    "Out[246]:\n",
    "239.3678426140957\n",
    "In [247]:\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "Out[247]:\n",
    "array([81.3704298 , 41.96610311, 49.94833006, 23.72101293, 73.76830889,\n",
    "       33.4770681 , 79.85000562, 38.29174467, 46.9074817 , 24.48122503,\n",
    "       73.51490486, 51.21535021, 42.34620915, 11.30421546, 22.83409883,\n",
    "       58.43736507, 31.95664392, 10.41730135, 82.76415197])\n",
    "In [248]:\n",
    "model.score(X_test, y_test) # метод возвращает значение коэффициента детерминации\n",
    "Out[248]:\n",
    "0.4948982822876836\n",
    "In [249]:\n",
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, c='r')\n",
    "# видим нелинейную регрессию.\n",
    "# связь между параметрами отсутствует\n",
    "Out[249]:\n",
    "[<matplotlib.lines.Line2D at 0x17c3e160>]\n",
    "\n",
    "In [250]:\n",
    "# ГРАФИК ОСТАТКОВ\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "In [251]:\n",
    "X_const = sm.add_constant(X_train) # техническая особенность библиотек, надо руками добавить константу\n",
    "In [252]:\n",
    "X_const.shape\n",
    "Out[252]:\n",
    "(42, 2)\n",
    "In [253]:\n",
    "model = sm.OLS(y_train, X_const)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:               hardness   R-squared:                       0.397\n",
    "Model:                            OLS   Adj. R-squared:                  0.382\n",
    "Method:                 Least Squares   F-statistic:                     26.31\n",
    "Date:                Sat, 21 Nov 2020   Prob (F-statistic):           7.83e-06\n",
    "Time:                        23:00:15   Log-Likelihood:                -201.74\n",
    "No. Observations:                  42   AIC:                             407.5\n",
    "Df Residuals:                      40   BIC:                             411.0\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const        239.3678     37.934      6.310      0.000     162.700     316.035\n",
    "mortality     -0.1267      0.025     -5.130      0.000      -0.177      -0.077\n",
    "==============================================================================\n",
    "Omnibus:                        0.559   Durbin-Watson:                   2.368\n",
    "Prob(Omnibus):                  0.756   Jarque-Bera (JB):                0.689\n",
    "Skew:                           0.197   Prob(JB):                        0.708\n",
    "Kurtosis:                       2.511   Cond. No.                     1.25e+04\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The condition number is large, 1.25e+04. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems.\n",
    "In [254]:\n",
    "# посмотрим на остатки\n",
    "plt.scatter(X_const.iloc[:, 1], results.resid)\n",
    "Out[254]:\n",
    "<matplotlib.collections.PathCollection at 0x17c6cdc0>\n",
    "\n",
    "In [255]:\n",
    "plt.hist(results.resid)\n",
    "Out[255]:\n",
    "(array([3., 1., 9., 5., 6., 7., 5., 2., 3., 1.]),\n",
    " array([-58.7233036 , -45.75983806, -32.79637252, -19.83290698,\n",
    "         -6.86944144,   6.0940241 ,  19.05748964,  32.02095518,\n",
    "         44.98442072,  57.94788627,  70.91135181]),\n",
    " <a list of 10 Patch objects>)\n",
    "\n",
    "south location\n",
    "\n",
    "In [ ]:\n",
    "#СДЕЛАЕМ ДАТАФРЕЙМ С ОТФИЛЬТРОВАННЫМИ ДАННЫМИ South Location\n",
    "df_south = df.loc[(df.location == 'South')]\n",
    "df_south.head()\n",
    "In [257]:\n",
    "df_south.plot(kind='scatter', x='mortality', y='hardness')\n",
    "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
    "Out[257]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x17ce7be0>\n",
    "\n",
    "In [258]:\n",
    "sns.pairplot(df_south) # используем pairplot для изучения взаимосвязи  признаков в South Location\n",
    "Out[258]:\n",
    "<seaborn.axisgrid.PairGrid at 0x17d16d30>\n",
    "\n",
    "In [259]:\n",
    "# Рассчитаем коэффициент корреляции Пирсона в South Location\n",
    "df_south[['mortality','hardness']].corr\n",
    "Out[259]:\n",
    "<bound method DataFrame.corr of     mortality  hardness\n",
    "0        1247       105\n",
    "2        1466         5\n",
    "7        1299        78\n",
    "9        1359        84\n",
    "10       1392        73\n",
    "12       1519        21\n",
    "13       1307        78\n",
    "14       1254        96\n",
    "18       1318       122\n",
    "19       1260        21\n",
    "25       1096       138\n",
    "27       1402        37\n",
    "32       1581        14\n",
    "33       1309        59\n",
    "34       1259       133\n",
    "37       1175       107\n",
    "38       1486         5\n",
    "39       1456        90\n",
    "41       1236       101\n",
    "47       1369        68\n",
    "48       1257        50\n",
    "54       1625        13\n",
    "56       1527        60\n",
    "57       1627        53\n",
    "58       1486       122\n",
    "59       1485        81>\n",
    "In [260]:\n",
    "# Рассчитаем коэффициент корреляции Спирмена в South Location\n",
    "df_south[['mortality','hardness']].corr(method = 'spearman')\n",
    "# видим разницу в меньшую сторону по абсолютному значению величины  по сравнению с общим датафреймом df,\n",
    "#где присуствовали данными по обоим регионам South и North Location ( |-0,59|<|-0,63|)\n",
    "Out[260]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.595723\n",
    "hardness\t-0.595723\t1.000000\n",
    "In [161]:\n",
    "# Рассчитать коэффициент корреляции Кенделла в South Location\n",
    "df_south[['mortality','hardness']].corr(method = 'kendall')\n",
    "# видим разницу в меньшую сторону по абсолютному значению величины  по сравнению с общим датафреймом df,\n",
    "#где присуствовали данными по обоим регионам South и North Location (|-0,44|)<|-0,45|)\n",
    "Out[161]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.440315\n",
    "hardness\t-0.440315\t1.000000\n",
    "In [283]:\n",
    "# строим тепловую карту South Location\n",
    "sns.set(rc={'figure.figsize':(5.0,4.0)})\n",
    "sns.heatmap(df_south.corr(),annot= True)\n",
    "Out[283]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x18521df0>\n",
    "\n",
    "In [306]:\n",
    "# Построим модель линейной регрессии для South Location\n",
    "X1 = df_south[['mortality']]\n",
    "y1 = df_south['hardness']\n",
    "In [307]:\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.30, random_state=42)\n",
    "In [308]:\n",
    "X1_train.shape\n",
    "Out[308]:\n",
    "(18, 1)\n",
    "In [309]:\n",
    "y1_train.shape\n",
    "Out[309]:\n",
    "(18,)\n",
    "In [310]:\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X1_train, y1_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[310]:\n",
    "LinearRegression()\n",
    "In [311]:\n",
    "model1.coef_\n",
    "Out[311]:\n",
    "array([-0.18112812])\n",
    "In [312]:\n",
    "model1.intercept_\n",
    "Out[312]:\n",
    "324.11907462359864\n",
    "In [313]:\n",
    "y1_pred = model1.predict(X1_test)\n",
    "y1_pred\n",
    "Out[313]:\n",
    "array([85.39221138, 54.96268708, 98.25230796, 54.96268708, 70.17744923,\n",
    "       95.89764238, 87.02236446, 58.58524949])\n",
    "In [314]:\n",
    "model1.score(X1_test, y1_test) # метод возвращает значение коэффициента детерминации\n",
    "Out[314]:\n",
    "-0.05226615794483824\n",
    "In [315]:\n",
    "plt.scatter(X1_test, y1_test)\n",
    "plt.plot(X1_test, y1_pred, c='r')\n",
    "# видим нелинейную регрессию.\n",
    "# связь между параметрами отсутствует\n",
    "Out[315]:\n",
    "[<matplotlib.lines.Line2D at 0x19ce1e50>]\n",
    "\n",
    "In [316]:\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "In [317]:\n",
    "X1_const = sm.add_constant(X1_train) # техническая особенность библиотек, надо руками добавить константу\n",
    "In [318]:\n",
    "X1_const.shape\n",
    "Out[318]:\n",
    "(18, 2)\n",
    "In [291]:\n",
    "model1 = sm.OLS(y1_train, X1_const)\n",
    "results = model1.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:               hardness   R-squared:                       0.636\n",
    "Model:                            OLS   Adj. R-squared:                  0.614\n",
    "Method:                 Least Squares   F-statistic:                     28.00\n",
    "Date:                Sat, 21 Nov 2020   Prob (F-statistic):           7.30e-05\n",
    "Time:                        23:15:49   Log-Likelihood:                -80.286\n",
    "No. Observations:                  18   AIC:                             164.6\n",
    "Df Residuals:                      16   BIC:                             166.4\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const        324.1191     47.493      6.825      0.000     223.439     424.799\n",
    "mortality     -0.1811      0.034     -5.292      0.000      -0.254      -0.109\n",
    "==============================================================================\n",
    "Omnibus:                        0.204   Durbin-Watson:                   2.189\n",
    "Prob(Omnibus):                  0.903   Jarque-Bera (JB):                0.212\n",
    "Skew:                          -0.195   Prob(JB):                        0.899\n",
    "Kurtosis:                       2.638   Cond. No.                     1.26e+04\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The condition number is large, 1.26e+04. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems.\n",
    "C:\\Users\\Admin\\anaconda\\lib\\site-packages\\scipy\\stats\\stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
    "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
    "In [319]:\n",
    "# посмотрим на остатки\n",
    "plt.scatter(X1_const.iloc[:, 1], results.resid)\n",
    "Out[319]:\n",
    "<matplotlib.collections.PathCollection at 0x19d11b80>\n",
    "\n",
    "In [320]:\n",
    "plt.hist(results.resid)\n",
    "Out[320]:\n",
    "(array([1., 0., 2., 1., 3., 4., 1., 2., 2., 2.]),\n",
    " array([-46.44102675, -38.10480112, -29.7685755 , -21.43234987,\n",
    "        -13.09612425,  -4.75989863,   3.576327  ,  11.91255262,\n",
    "         20.24877825,  28.58500387,  36.92122949]),\n",
    " <a list of 10 Patch objects>)\n",
    "\n",
    "North Location\n",
    "\n",
    "In [343]:\n",
    "#СДЕЛАЕМ ДАТАФРЕЙМ С ОТФИЛЬТРОВАННЫМИ ДАННЫМИ North Location\n",
    "df_north = df.loc[(df.location == 'North')]\n",
    "df_north.head()\n",
    "Out[343]:\n",
    "Unnamed: 0\tlocation\ttown\tmortality\thardness\n",
    "1\t2\tNorth\tBirkenhead\t1668\t17\n",
    "3\t4\tNorth\tBlackburn\t1800\t14\n",
    "4\t5\tNorth\tBlackpool\t1609\t18\n",
    "5\t6\tNorth\tBolton\t1558\t10\n",
    "6\t7\tNorth\tBootle\t1807\t15\n",
    "In [344]:\n",
    "df_north.plot(kind='scatter', x='mortality', y='hardness')\n",
    "*c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2-D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
    "Out[344]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x1a011880>\n",
    "\n",
    "In [296]:\n",
    "sns.pairplot(df_north) # используем pairplot для изучения взаимосвязи признаков в North Location\n",
    "# Видим, что графики чуть отличаются от аналогичных графиков South Location, в более линейную сторону \n",
    "# по нижней границу hardness до 20,\n",
    "# но по итогу взаимосвязь между жесткостью воды и смертностью - тоже явно не прослеживается\n",
    "Out[296]:\n",
    "<seaborn.axisgrid.PairGrid at 0x195bc790>\n",
    "\n",
    "In [297]:\n",
    "# Рассчитаем коэффициент корреляции Пирсона в North Location\n",
    "df_north[['mortality','hardness']].corr\n",
    "Out[297]:\n",
    "<bound method DataFrame.corr of     mortality  hardness\n",
    "1        1668        17\n",
    "3        1800        14\n",
    "4        1609        18\n",
    "5        1558        10\n",
    "6        1807        15\n",
    "8        1637        10\n",
    "11       1755        12\n",
    "15       1491        20\n",
    "16       1555        39\n",
    "17       1428        39\n",
    "20       1723        44\n",
    "21       1379        94\n",
    "22       1742         8\n",
    "23       1574         9\n",
    "24       1569        91\n",
    "26       1591        16\n",
    "28       1772        15\n",
    "29       1828         8\n",
    "30       1704        26\n",
    "31       1702        44\n",
    "35       1427        27\n",
    "36       1724         6\n",
    "40       1696         6\n",
    "42       1711        13\n",
    "43       1444        14\n",
    "44       1591        49\n",
    "45       1987         8\n",
    "46       1495        14\n",
    "49       1587        75\n",
    "50       1713        71\n",
    "51       1557        13\n",
    "52       1640        57\n",
    "53       1709        71\n",
    "55       1625        20\n",
    "60       1378        71>\n",
    "In [298]:\n",
    "# Рассчитаем коэффициент корреляции Спирмена в North Location\n",
    "df_north[['mortality','hardness']].corr(method = 'spearman')\n",
    "# видим разницу в меньшую сторону по абсолютному значению величины  по сравнению с общим датафреймом df,\n",
    "#где присуствовали данными по обоим регионам South и North Location (|-0,40|<|-0,44|)\n",
    "# и в еще меньшую сторону относительно аналогичного расчета по South Location (|-0,40|<|-0,45|)\n",
    "Out[298]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.404208\n",
    "hardness\t-0.404208\t1.000000\n",
    "In [299]:\n",
    "# Рассчитаем коэффициент корреляции Кенделла в North Location\n",
    "df_north[['mortality','hardness']].corr(method = 'kendall')\n",
    "# видим разницу в меньшую сторону по абсолютному значению величины по сравнению с общим датафреймом df,\n",
    "#где присуствовали данными по обоим регионам South и North Location (|-0,28|<|-0,60|)\n",
    "# и в еще меньшую сторону относительно аналогичного расчета по South Location (|-0,28|<|-0,60|)\n",
    "Out[299]:\n",
    "mortality\thardness\n",
    "mortality\t1.000000\t-0.283058\n",
    "hardness\t-0.283058\t1.000000\n",
    "In [300]:\n",
    "# строим тепловую карту North Location\n",
    "sns.set(rc={'figure.figsize':(5.0,4.0)})\n",
    "sns.heatmap(df_north.corr(),annot= True)\n",
    "Out[300]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x199863a0>\n",
    "\n",
    "In [345]:\n",
    "# Построим модель линейной регрессии для North Location\n",
    "X2 = df_north[['mortality']]\n",
    "y2 = df_north['hardness']\n",
    "In [346]:\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.30, random_state=42)\n",
    "In [347]:\n",
    "X2_train.shape\n",
    "Out[347]:\n",
    "(24, 1)\n",
    "In [348]:\n",
    "y2_train.shape\n",
    "Out[348]:\n",
    "(24,)\n",
    "In [349]:\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X2_train, y2_train)  # метод обучается на данных и подбирает оптимальные коэффициенты\n",
    "Out[349]:\n",
    "LinearRegression()\n",
    "In [350]:\n",
    "model2.coef_\n",
    "Out[350]:\n",
    "array([-0.09774218])\n",
    "In [351]:\n",
    "model2.intercept_\n",
    "Out[351]:\n",
    "191.72797679112006\n",
    "In [352]:\n",
    "y2_pred = model2.predict(X2_test)\n",
    "y2_pred\n",
    "Out[352]:\n",
    "array([-2.48573411, 37.88178607, 50.58826942, 23.22045913, 36.22016902,\n",
    "       24.29562311, 25.37078708, 21.4610999 , 39.73888749, 18.52883451,\n",
    "       52.1521443 ])\n",
    "In [353]:\n",
    "model2.score(X2_test, y2_test) # метод возвращает значение коэффициента детерминации\n",
    "Out[353]:\n",
    "-0.34863672627416675\n",
    "In [354]:\n",
    "plt.scatter(X2_test, y2_test)\n",
    "plt.plot(X2_test, y2_pred, c='r')\n",
    "# видим нелинейную регрессию.\n",
    "# связь между параметрами также отсутствует\n",
    "Out[354]:\n",
    "[<matplotlib.lines.Line2D at 0x1a080b50>]\n",
    "\n",
    "In [355]:\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "In [356]:\n",
    "X2_const = sm.add_constant(X2_train) # техническая особенность библиотек, надо руками добавить константу\n",
    "In [357]:\n",
    "X2_const.shape\n",
    "Out[357]:\n",
    "(24, 2)\n",
    "In [358]:\n",
    "model2 = sm.OLS(y2_train, X2_const)\n",
    "results = model2.fit()\n",
    "print(results.summary())\n",
    "                            OLS Regression Results                            \n",
    "==============================================================================\n",
    "Dep. Variable:               hardness   R-squared:                       0.193\n",
    "Model:                            OLS   Adj. R-squared:                  0.157\n",
    "Method:                 Least Squares   F-statistic:                     5.269\n",
    "Date:                Sat, 21 Nov 2020   Prob (F-statistic):             0.0316\n",
    "Time:                        23:45:25   Log-Likelihood:                -111.11\n",
    "No. Observations:                  24   AIC:                             226.2\n",
    "Df Residuals:                      22   BIC:                             228.6\n",
    "Df Model:                           1                                         \n",
    "Covariance Type:            nonrobust                                         \n",
    "==============================================================================\n",
    "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const        191.7280     69.297      2.767      0.011      48.015     335.441\n",
    "mortality     -0.0977      0.043     -2.295      0.032      -0.186      -0.009\n",
    "==============================================================================\n",
    "Omnibus:                        2.755   Durbin-Watson:                   2.028\n",
    "Prob(Omnibus):                  0.252   Jarque-Bera (JB):                2.318\n",
    "Skew:                           0.669   Prob(JB):                        0.314\n",
    "Kurtosis:                       2.274   Cond. No.                     2.13e+04\n",
    "==============================================================================\n",
    "\n",
    "Warnings:\n",
    "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
    "[2] The condition number is large, 2.13e+04. This might indicate that there are\n",
    "strong multicollinearity or other numerical problems.\n",
    "In [359]:\n",
    "# посмотрим на остатки\n",
    "plt.scatter(X2_const.iloc[:, 1], results.resid)\n",
    "Out[359]:\n",
    "<matplotlib.collections.PathCollection at 0x1a05a880>\n",
    "\n",
    "In [360]:\n",
    "plt.hist(results.resid)\n",
    "Out[360]:\n",
    "(array([5., 3., 4., 4., 0., 2., 2., 0., 2., 2.]),\n",
    " array([-31.60341826, -23.18012614, -14.75683401,  -6.33354188,\n",
    "          2.08975025,  10.51304238,  18.93633451,  27.35962664,\n",
    "         35.78291877,  44.2062109 ,  52.62950303]),\n",
    " <a list of 10 Patch objects>)\n",
    "\n",
    "In [ ]:\n",
    "#ОТВЕТ: линейной зависимости жёсткостью воды и средней годовой смертностью - нет, \n",
    "#ни по отношению к совокупности South и North Location, ни для каждого их них в отдельности"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
